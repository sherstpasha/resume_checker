import os
import threading
import time
from typing import List

import numpy as np
import requests
import sounddevice as sd
import torch
import whisper
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, Request, UploadFile, File
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
import tempfile
from silero_vad import load_silero_vad, get_speech_timestamps
from utils import extract_text, load_job_requirements, strip_thinking_tags
import json
from utils import FallbackTTS
from agents.resume_analyzer import ResumeAnalyzerAgent


STOP_WORDS = ["—Å—Ç–æ–ø", "–∑–∞–∫–æ–Ω—á–∏–º", "—Ö–≤–∞—Ç–∏—Ç", "–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ", "–∑–∞–≤–µ—Ä—à–∏–º"]

# ============================ –ö–æ–Ω—Ñ–∏–≥ ============================

load_dotenv()
API_BASE_URL = os.getenv("API_BASE_URL", "http://127.0.0.1:8080/v1")
MODEL_NAME = os.getenv("MODEL_NAME", "llama-2-7b-chat")
JOB_DESCRIPTION_PATH = os.getenv("JOB_DESCRIPTION_PATH")
resume_agent = ResumeAnalyzerAgent(API_BASE_URL, MODEL_NAME)

SAMPLERATE = 16000
CHUNK_SEC = 1
PAUSE_CHUNKS = 2

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
if DEVICE == "cuda":
    torch.set_float32_matmul_precision("high")

# ============================ –°–æ—Å—Ç–æ—è–Ω–∏–µ ============================

app = FastAPI()
templates = Jinja2Templates(directory="templates")

_interview_running = False
_speaking = False
_log: List[str] = []
_lock = threading.Lock()
_system_prompt = None
_dialogue_history: List[dict] = []

# ============================ –ú–æ–¥–µ–ª–∏ ============================

vad_model = load_silero_vad(onnx=False)
_VAD_DEVICE = "cpu"

stt_model = whisper.load_model("small", device=DEVICE)

tts = FallbackTTS()

# ============================ –£—Ç–∏–ª–∏—Ç—ã ============================


def log_line(text: str):
    with _lock:
        print(text)
        _log.append(text)


def check_end_dialogue(user_text: str, assistant_text: str) -> bool:
    try:
        prompt = f"""
–¢—ã –±–∏–Ω–∞—Ä–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä. 
–í—Ö–æ–¥: —Ä–µ–ø–ª–∏–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∏ –æ—Ç–≤–µ—Ç –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞.
–û–ø—Ä–µ–¥–µ–ª–∏, –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å –ª–∏ –∏–Ω—Ç–µ—Ä–≤—å—é. 

–ü—Ä–∞–≤–∏–ª–∞:
- –ï—Å–ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç —è–≤–Ω–æ —Å–∫–∞–∑–∞–ª "—Å—Ç–æ–ø", "–∑–∞–∫–æ–Ω—á–∏–º", "—Ö–≤–∞—Ç–∏—Ç" –∏ —Ç.–ø. ‚Üí end=1
- –ï—Å–ª–∏ –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä —Å–∫–∞–∑–∞–ª, —á—Ç–æ –≤—Å—ë –∑–∞–≤–µ—Ä—à–µ–Ω–æ ‚Üí end=1
- –ò–Ω–∞—á–µ ‚Üí end=0

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ —Å—Ç—Ä–æ–≥–æ JSON:
{{"end": 0}} –∏–ª–∏ {{"end": 1}}

–†–µ–ø–ª–∏–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:
{user_text}

–†–µ–ø–ª–∏–∫–∞ –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞:
{assistant_text}
"""
        payload = {
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": prompt}],
        }
        r = requests.post(f"{API_BASE_URL}/chat/completions", json=payload, timeout=30)
        r.raise_for_status()
        raw = r.json()["choices"][0]["message"]["content"]
        parsed = json.loads(strip_thinking_tags(raw))
        return parsed.get("end", 0) == 1
    except Exception as e:
        log_line(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ check_end_dialogue: {e}")
        return False


def ask_llm(user_text: str) -> str:
    global _system_prompt, _dialogue_history, _interview_running

    # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
    if not _dialogue_history:
        _dialogue_history.append({"role": "system", "content": _system_prompt})

    # –¥–æ–±–∞–≤–ª—è–µ–º —Ä–µ–ø–ª–∏–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    _dialogue_history.append({"role": "user", "content": user_text})

    try:
        payload = {
            "model": MODEL_NAME,
            "messages": _dialogue_history,
            "temperature": 0.4,
        }
        r = requests.post(f"{API_BASE_URL}/chat/completions", json=payload, timeout=60)
        r.raise_for_status()
        raw = r.json()["choices"][0]["message"]["content"]

        # ‚úÖ –æ—á–∏—â–∞–µ–º –ª–∏—à–Ω–∏–µ —Ç–µ–≥–∏ (chain-of-thought, <|channel|> –∏ –ø—Ä.)
        clean = strip_thinking_tags(raw)

        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        _dialogue_history.append({"role": "assistant", "content": clean})

        # –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –∏–Ω—Ç–µ—Ä–≤—å—é
        if check_end_dialogue(user_text, clean):
            log_line("üìä –ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ä–µ—à–∏–ª–∞: –∏–Ω—Ç–µ—Ä–≤—å—é –∑–∞–∫–æ–Ω—á–µ–Ω–æ.")

            # –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
            prompt_report = """
            –ù–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ–≥–æ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ 
            —Å—Ñ–æ—Ä–º–∏—Ä—É–π –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
            {
              "–ö—Ä–∏—Ç–µ—Ä–∏–∏": {
                "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏": "–æ—Ü–µ–Ω–∫–∞ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π",
                "–ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è": "–æ—Ü–µ–Ω–∫–∞ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π",
                "–ú–æ—Ç–∏–≤–∞—Ü–∏—è –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª": "–æ—Ü–µ–Ω–∫–∞ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π"
              },
              "–û–±—â–∏–π –≤—ã–≤–æ–¥": "—Ç–µ–∫—Å—Ç–æ–≤–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ"
            }
            """
            payload = {
                "model": MODEL_NAME,
                "messages": [
                    {"role": "system", "content": _system_prompt},
                    {"role": "user", "content": prompt_report},
                ],
            }
            r = requests.post(
                f"{API_BASE_URL}/chat/completions", json=payload, timeout=60
            )
            r.raise_for_status()
            report_raw = r.json()["choices"][0]["message"]["content"]

            # —Ç–æ–∂–µ —á–∏—Å—Ç–∏–º
            clean_report = strip_thinking_tags(report_raw)
            log_line("üìä –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç:\n" + clean_report)

            _interview_running = False
            return "–°–ø–∞—Å–∏–±–æ –∑–∞ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ! –ú—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç."

        return clean

    except Exception as e:
        log_line(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ ask_llm: {e}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ç–≤–µ—Ç–∞."


# ============================ –õ–æ–≥–∏–∫–∞ –∏–Ω—Ç–µ—Ä–≤—å—é ============================


def interview_loop():
    global _interview_running, _speaking

    log_line(f"üñ•Ô∏è Whisper: {DEVICE} | TTS: {DEVICE} | VAD: {_VAD_DEVICE}")
    buffer = []
    silence_count = 0
    log_line("üé§ –°–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–æ—Å—å. –ì–æ–≤–æ—Ä–∏—Ç–µ...")

    while _interview_running:
        if _speaking:
            buffer = []
            silence_count = 0
            time.sleep(0.05)
            continue

        audio = sd.rec(
            int(SAMPLERATE * CHUNK_SEC),
            samplerate=SAMPLERATE,
            channels=1,
            dtype="float32",
        )
        sd.wait()
        audio = audio.flatten()

        x = torch.tensor(audio, dtype=torch.float32, device=_VAD_DEVICE)
        speech_ts = get_speech_timestamps(x, vad_model, sampling_rate=SAMPLERATE)

        if len(speech_ts) == 0:
            silence_count += 1
            if silence_count >= PAUSE_CHUNKS and len(buffer) > 0:
                chunk = np.concatenate(buffer)
                if np.max(np.abs(chunk)) > 0:
                    chunk = chunk / np.max(np.abs(chunk))

                result = stt_model.transcribe(
                    chunk, language="ru", fp16=(DEVICE == "cuda")
                )
                user_text = result["text"].strip()
                log_line(f"üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_text}")

                try:
                    assistant_text = ask_llm(user_text)
                    log_line(f"ü§ñ –ò–Ω—Ç–µ—Ä–≤—å—é–µ—Ä: {assistant_text}")
                    _speaking = True
                    tts.speak(assistant_text)
                    _speaking = False
                except Exception as e:
                    log_line(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ LLM/TTS: {e}")

                buffer = []
                silence_count = 0
        else:
            silence_count = 0
            buffer.append(audio)

        time.sleep(0.01)

    log_line("‚èπ –°–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.")


# ============================ –†–æ—É—Ç—ã ============================


@app.get("/", response_class=HTMLResponse)
def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})


JOB_REQUIREMENTS = load_job_requirements(JOB_DESCRIPTION_PATH)


@app.post("/api/upload_resume")
async def upload_resume(file: UploadFile = File(...)):
    global _system_prompt
    try:
        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        tmp_dir = tempfile.gettempdir()
        save_path = os.path.join(tmp_dir, file.filename)
        with open(save_path, "wb") as f:
            f.write(await file.read())

        # –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ
        resume_text = extract_text(save_path)
        if not resume_text.strip():
            return JSONResponse(
                {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞."}, status_code=400
            )

        # --- –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–∞ ---
        result = resume_agent.analyze_and_questions(resume_text, JOB_REQUIREMENTS)

        analysis = result.get("–ê–Ω–∞–ª–∏–∑", {})
        questions = result.get("–í–æ–ø—Ä–æ—Å—ã", [])

        # --- —Å–æ–±–∏—Ä–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞ ---
        _system_prompt = f"""
–¢—ã –∏–≥—Ä–∞–µ—à—å —Ä–æ–ª—å –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞. 
–¢–≤–æ—è —Ü–µ–ª—å ‚Äì –≤–µ—Å—Ç–∏ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ –∏ —Å–ø–∏—Å–æ–∫ –∑–∞—Ä–∞–Ω–µ–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.

–ê–Ω–∞–ª–∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:
{json.dumps(analysis, ensure_ascii=False, indent=2)}

–°–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤:
{json.dumps(questions, ensure_ascii=False, indent=2)}

–ü—Ä–∞–≤–∏–ª–∞:
- –ì–æ–≤–æ—Ä–∏ —Ç–æ–ª—å–∫–æ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ —Ä–µ–ø–ª–∏–∫–∞–º–∏.
- –ó–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å—ã –ø–æ –ø–æ—Ä—è–¥–∫—É, –Ω–∞—á–∏–Ω–∞—è —Å –ø–µ—Ä–≤–æ–≥–æ.
- ‚ö†Ô∏è –í—Å–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å–ª–æ–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä –Ω–∞–∑–≤–∞–Ω–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏–ª–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π) –ø–∏—à–∏ –≤ —Ä—É—Å—Å–∫–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏,
  —á—Ç–æ–±—ã —Å–∏–Ω—Ç–µ–∑–∞—Ç–æ—Ä —Ä–µ—á–∏ –ø—Ä–æ–∏–∑–Ω—ë—Å –∏—Ö –ø—Ä–∞–≤–∏–ª—å–Ω–æ. –ü—Ä–∏–º–µ—Ä: "TensorFlow" ‚Üí "–¢√©–Ω—Å–æ—Ä—Ñ–ª–æ—É", "PyTorch" ‚Üí "–ü–∞–π—Ç√≥—Ä—á".
"""

        return JSONResponse({"analysis": analysis, "questions": questions})

    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


@app.post("/api/start")
def api_start():
    global _interview_running, _log, _system_prompt, _dialogue_history
    if not _system_prompt:
        return JSONResponse(
            {"status": "error", "message": "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ–∑—é–º–µ"}, status_code=400
        )

    with _lock:
        if not _interview_running:
            _log = []
            _dialogue_history = []
            _interview_running = True
            threading.Thread(target=interview_loop, daemon=True).start()
            return JSONResponse({"status": "started"})
    return JSONResponse({"status": "already running"})


@app.post("/api/stop")
def api_stop():
    global _interview_running
    with _lock:
        _interview_running = False
    return JSONResponse({"status": "stopped"})


@app.get("/api/speaking")
def api_speaking():
    return JSONResponse({"speaking": _speaking})


@app.get("/api/logs")
def api_logs():
    with _lock:
        return JSONResponse({"dialogue": "\n".join(_log)})


# ============================ –ó–∞–ø—É—Å–∫ ============================

if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=8081)
