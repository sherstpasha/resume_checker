# run_interview_cli.py
import os
import time
import json
import torch
import numpy as np
import sounddevice as sd
from dotenv import load_dotenv

from silero_vad import load_silero_vad, get_speech_timestamps
from utils import extract_text, load_job_requirements
from agents.resume_analyzer import ResumeAnalyzerAgent
from agents.interviewer import HRInterviewerAgent

# ============================ –ö–û–ù–§–ò–ì ============================

# –£–∫–∞–∂–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Ä–µ–∑—é–º–µ:
RESUME_PATH = r"C:\Users\pasha\Downloads\Telegram Desktop\ML_Engineer_–®–µ—Ä—Å—Ç–Ω–µ–≤_–ü–∞–≤–µ–ª (2).pdf"  # <<< –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–π –ø—É—Ç—å

STOP_WORDS = ["—Å—Ç–æ–ø", "–∑–∞–∫–æ–Ω—á–∏–º", "—Ö–≤–∞—Ç–∏—Ç", "–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ", "–∑–∞–≤–µ—Ä—à–∏–º"]

load_dotenv()
API_BASE_URL = os.getenv("API_BASE_URL", "http://127.0.0.1:8080/v1")
MODEL_NAME = os.getenv("MODEL_NAME", "llama-2-7b-chat")
JOB_DESCRIPTION_PATH = os.getenv("JOB_DESCRIPTION_PATH")

SAMPLERATE = 16000
CHUNK_SEC = 1
PAUSE_CHUNKS = 2

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
if DEVICE == "cuda":
    torch.set_float32_matmul_precision("high")

# ============================ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ============================
print("‚è≥ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è VAD...")
vad_model = load_silero_vad(onnx=False)
_VAD_DEVICE = "cpu"


def vad_listen_loop(interviewer: HRInterviewerAgent):
    """–°–ª—É—à–∞–µ–º –º–∏–∫—Ä–æ—Ñ–æ–Ω –∫—É—Å–∫–∞–º–∏, –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –ø–∞—É–∑—ã VAD,
    –ø—Ä–∏ –ø–∞—É–∑–µ ‚Äî —Å–∫–ª–µ–∏–≤–∞–µ–º –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞—ë–º —á–µ—Ä–µ–∑ interviewer.stt_model."""
    print(f"üñ•Ô∏è Whisper device: {DEVICE} | VAD: {_VAD_DEVICE}")
    print("üé§ –°–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–æ—Å—å. –ì–æ–≤–æ—Ä–∏—Ç–µ... (—Å–∫–∞–∂–∏—Ç–µ ¬´—Å—Ç–æ–ø¬ª —á—Ç–æ–±—ã –∑–∞–≤–µ—Ä—à–∏—Ç—å)")
    buffer = []
    silence_count = 0

    while not interviewer.finished:
        # –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—á–µ—Ä–µ–¥–Ω–æ–π —Ñ—Ä–µ–π–º
        audio = sd.rec(
            int(SAMPLERATE * CHUNK_SEC),
            samplerate=SAMPLERATE,
            channels=1,
            dtype="float32",
        )
        sd.wait()
        audio = audio.flatten()

        # –ø—Ä–æ–≤–µ—Ä—è–µ–º VAD
        x = torch.tensor(audio, dtype=torch.float32, device=_VAD_DEVICE)
        speech_ts = get_speech_timestamps(x, vad_model, sampling_rate=SAMPLERATE)

        if len(speech_ts) == 0:
            silence_count += 1
            if silence_count >= PAUSE_CHUNKS and len(buffer) > 0:
                # —Å–∫–ª–µ–∏–≤–∞–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–µ
                chunk = np.concatenate(buffer)
                buffer = []
                silence_count = 0

                if np.max(np.abs(chunk)) > 0:
                    chunk = chunk / np.max(np.abs(chunk))

                # —Ä–∞—Å–ø–æ–∑–Ω–∞—ë–º —á–µ—Ä–µ–∑ STT –∏–∑ –∞–≥–µ–Ω—Ç–∞ (—É –Ω–µ–≥–æ —É–∂–µ –µ—Å—Ç—å stt_model)
                result = interviewer.stt_model.transcribe(
                    chunk, language="ru", fp16=(DEVICE == "cuda")
                )
                user_text = result["text"].strip()
                if not user_text:
                    continue

                print(f"üë§ –ö–∞–Ω–¥–∏–¥–∞—Ç: {user_text}")

                # –±—ã—Å—Ç—Ä—ã–π –≤—ã—Ö–æ–¥ –ø–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º
                if any(sw in user_text.lower() for sw in STOP_WORDS):
                    break

                # –æ—Ç–≤–µ—Ç –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞ (LLM –≤–Ω—É—Ç—Ä–∏ –∞–≥–µ–Ω—Ç–∞)
                reply = interviewer.reply(user_text)
                print(f"ü§ñ –ò–Ω—Ç–µ—Ä–≤—å—é–µ—Ä: {reply}")

                # –æ–∑–≤—É—á–∏–≤–∞–µ–º –æ—Ç–≤–µ—Ç –∞–≥–µ–Ω—Ç—Å–∫–∏–º TTS
                interviewer.tts.speak(reply)
        else:
            silence_count = 0
            buffer.append(audio)

        time.sleep(0.01)

    print("‚èπ –°–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.")


def main():
    # 1) –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—é–º–µ –∏ –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç
    if not RESUME_PATH or not os.path.exists(RESUME_PATH):
        print(f"‚ùå RESUME_PATH –Ω–µ –Ω–∞–π–¥–µ–Ω: {RESUME_PATH}")
        return

    print("üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—é–º–µ:", RESUME_PATH)
    resume_text = extract_text(RESUME_PATH)
    if not resume_text.strip():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞.")
        return

    # 2) —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
    job_requirements = load_job_requirements(JOB_DESCRIPTION_PATH)

    # 3) –∞–Ω–∞–ª–∏–∑ + –≤–æ–ø—Ä–æ—Å—ã (–µ–¥–∏–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º)
    print("üîé –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ...")
    analyzer = ResumeAnalyzerAgent(API_BASE_URL, MODEL_NAME)
    result = analyzer.analyze_and_questions(resume_text, job_requirements)

    analysis = result.get("–ê–Ω–∞–ª–∏–∑", {})
    questions = result.get("–í–æ–ø—Ä–æ—Å—ã", [])

    print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
    print(json.dumps(analysis, ensure_ascii=False, indent=2))
    print("\n‚ùì –í–æ–ø—Ä–æ—Å—ã/—Ç–µ–º—ã –¥–ª—è –∏–Ω—Ç–µ—Ä–≤—å—é:")
    for q in questions:
        print("-", q)

    # 4) —Å–æ–∑–¥–∞—ë–º HR-–∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞ (–≤—Å—è LLM-–ª–æ–≥–∏–∫–∞ –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Å–∞)
    interviewer = HRInterviewerAgent(
        api_base_url=API_BASE_URL,
        model_name=MODEL_NAME,
        analysis=analysis,
        questions=questions,
        device=DEVICE,
    )

    # 5) –≥–æ–ª–æ—Å–æ–≤–æ–π —Ü–∏–∫–ª —Å VAD ‚Üí Whisper ‚Üí reply ‚Üí TTS
    vad_listen_loop(interviewer)

    # 6) —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
    print("\nüìë –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç...")
    report = interviewer.finish_and_report()
    print(json.dumps(report, ensure_ascii=False, indent=2))

    # 7) –∑–∞–º–µ—Ç–∫–∏ –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞ (–µ—Å–ª–∏ –±—É–¥–µ—Ç–µ –∏—Ö –∑–∞–ø–æ–ª–Ω—è—Ç—å interviewer.note(...))
    notes = interviewer.get_notes()
    if notes:
        print("\nüìù –ó–∞–º–µ—Ç–∫–∏ –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞:")
        for n in notes:
            print("-", n)


if __name__ == "__main__":
    main()
