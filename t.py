import os
import time
import json
import signal
import argparse

import numpy as np
import sounddevice as sd
import torch
import whisper
from dotenv import load_dotenv

from silero_vad import load_silero_vad, get_speech_timestamps


STOP_WORDS = ["ÑÑ‚Ğ¾Ğ¿", "Ğ·Ğ°ĞºĞ¾Ğ½Ñ‡Ğ¸Ğ¼", "Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚", "Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾", "Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ¼"]


def main():
    parser = argparse.ArgumentParser(
        description="Standalone mic â†’ VAD â†’ Whisper tester (CLI)"
    )
    parser.add_argument(
        "--model",
        default=os.getenv("WHISPER_MODEL", "medium"),
        help="whisper model (tiny/base/small/medium/large)",
    )
    parser.add_argument(
        "--device", default=os.getenv("STT_DEVICE", "auto"), help="auto|cuda|cpu"
    )
    parser.add_argument(
        "--samplerate", type=int, default=int(os.getenv("SAMPLERATE", "16000"))
    )
    parser.add_argument(
        "--chunk_sec", type=float, default=float(os.getenv("CHUNK_SEC", "1"))
    )
    parser.add_argument(
        "--pause_chunks", type=int, default=int(os.getenv("PAUSE_CHUNKS", "2"))
    )
    parser.add_argument(
        "--rms_gate",
        type=float,
        default=float(os.getenv("RMS_GATE", "0.03")),
        help="RMS threshold for fallback VAD",
    )
    parser.add_argument(
        "--device_index", type=int, default=None, help="sounddevice input device index"
    )
    parser.add_argument(
        "--list_devices", action="store_true", help="print sound devices and exit"
    )
    args = parser.parse_args()

    load_dotenv()

    if args.list_devices:
        print(sd.query_devices())
        return

    # Resolve torch device
    if args.device.lower() == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device.lower()
        if device == "cuda" and not torch.cuda.is_available():
            print("[WARN] CUDA requested but not available. Falling back to CPU.")
            device = "cpu"
    if device == "cuda":
        try:
            torch.set_float32_matmul_precision("high")
        except Exception:
            pass

    print("â³ Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ VADâ€¦")
    vad_model = load_silero_vad(onnx=False)
    _VAD_DEVICE = "cpu"
    print("âœ… VAD Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½ (ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: CPU)")

    print(f"â³ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Whisper ({args.model}) Ğ½Ğ° {device}â€¦")
    stt_model = whisper.load_model(args.model, device=device)
    print("âœ… Whisper Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½")

    samplerate = int(args.samplerate)
    chunk_sec = float(args.chunk_sec)
    pause_chunks = int(args.pause_chunks)
    rms_gate = float(args.rms_gate)

    print(f"ğŸ–¥ï¸ Whisper device: {device} | VAD: {_VAD_DEVICE}")
    print("ğŸ¤ Ğ¡Ğ¾Ğ±ĞµÑĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ÑÑŒ. Ğ“Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚Ğµâ€¦ (ÑĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ â€˜ÑÑ‚Ğ¾Ğ¿â€™ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ñ‚ÑŒ)")
    print(
        f"ğŸšï¸ SR={samplerate}Hz, Ğ¾ĞºĞ½Ğ¾={chunk_sec:.1f}s, Ğ¿Ğ°ÑƒĞ·Ğ°={pause_chunks} Ğ¾ĞºĞ½Ğ°, RMS_GATE={rms_gate}"
    )

    running = True

    def _sigint(_sig, _frm):
        nonlocal running
        running = False

    signal.signal(signal.SIGINT, _sigint)

    buffer = []
    silence_count = 0
    sd.default.samplerate = samplerate
    if args.device_index is not None:
        sd.default.device = (args.device_index, None)

    while running:
        # 1) Ğ·Ğ°Ğ¿Ğ¸ÑˆĞµĞ¼ 1 ÑĞµĞº
        audio = sd.rec(
            int(samplerate * chunk_sec),
            samplerate=samplerate,
            channels=1,
            dtype="float32",
        )
        sd.wait()
        audio = audio.flatten()

        # 2) VAD Ğ¿Ğ¾ ÑĞµĞºÑƒĞ½Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¾ĞºĞ½Ñƒ
        x = torch.tensor(audio, dtype=torch.float32, device=_VAD_DEVICE)
        try:
            ts = get_speech_timestamps(x, vad_model, sampling_rate=samplerate)
            has_speech_silero = len(ts) > 0
        except Exception:
            has_speech_silero = False

        rms = float(np.sqrt(np.mean(audio * audio)))
        has_speech_rms = rms > rms_gate
        has_speech = has_speech_silero or has_speech_rms
        print(
            f"â„¹ï¸ VAD 1s: rms={rms:.4f} has_speech={has_speech} (silero={has_speech_silero}, rms_gate={has_speech_rms})"
        )

        if has_speech:
            buffer.append(audio)
            silence_count = 0
        else:
            silence_count += 1

        # 3) ĞµÑĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ° Ñ€ĞµÑ‡ÑŒ Ğ¸ Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ¿Ğ°ÑƒĞ·Ğ° X*1s â†’ ASR
        if silence_count >= pause_chunks and buffer:
            chunk = np.concatenate(buffer)
            buffer.clear()
            silence_count = 0
            # Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
            m = float(np.max(np.abs(chunk))) if chunk.size else 0.0
            if m > 0:
                chunk = (chunk / m).astype(np.float32)
            dur = chunk.size / samplerate
            rms_seg = float(np.sqrt(np.mean(chunk * chunk))) if chunk.size else 0.0
            print(f"ğŸ™ï¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚ {dur:.2f}s, rms={rms_seg:.4f} â†’ ASR")
            if chunk.size < samplerate:  # Ğ¼ĞµĞ½ĞµĞµ 1 ÑĞµĞºÑƒĞ½Ğ´Ñ‹ â€” ÑĞºĞ¸Ğ¿Ğ°ĞµĞ¼
                continue
            try:
                result = stt_model.transcribe(
                    chunk, language="ru", fp16=(device == "cuda")
                )
                user_text = (result.get("text") or "").strip()
            except Exception as e:
                print(f"âš ï¸ Whisper error: {e}")
                user_text = ""
            print(f"ğŸ‘¤ ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ: {user_text}")
            if any(sw in user_text.lower() for sw in STOP_WORDS):
                break

        time.sleep(0.01)

    print("â¹ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°")


if __name__ == "__main__":
    main()
