import os
import gradio as gr
import requests
from dotenv import load_dotenv
from docx import Document
import textract
from pypdf import PdfReader
import json
import re

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()
API_BASE_URL = os.getenv("API_BASE_URL", "http://127.0.0.1:8080/v1").rstrip("/")
MODEL_NAME = os.getenv("MODEL_NAME", "llama-2-7b-chat")
JOB_DESCRIPTION_PATH = os.getenv("JOB_DESCRIPTION_PATH")

# ---------- –û—á–∏—Å—Ç–∫–∞ reasoning ----------
def strip_thinking_tags(text: str, tags=None) -> str:
    final_marker = "<|channel|>final<|message|>"
    if final_marker in text:
        text = text.split(final_marker, 1)[1]

    if tags is None:
        tags = ["think", "reflect", "reason"]

    for tag in tags:
        text = re.sub(rf"<{tag}>(.|\s)*?</{tag}>", "", text, flags=re.IGNORECASE)
        text = re.sub(rf"</?{tag}>", "", text, flags=re.IGNORECASE)

    text = re.sub(r"<\|[^>]+\|>", "", text)
    return "\n".join(line for line in text.splitlines() if line.strip()).strip()

# ---------- –ü–∞—Ä—Å–µ—Ä—ã —Ñ–∞–π–ª–æ–≤ ----------
def extract_text_from_pdf(path):
    text = ""
    reader = PdfReader(path)
    for page in reader.pages:
        if page.extract_text():
            text += page.extract_text() + "\n"
    return text.strip()

def extract_text_from_docx(path):
    texts = []
    try:
        doc = Document(path)
        for p in doc.paragraphs:
            if p.text.strip():
                texts.append(p.text.strip())
        for table in doc.tables:
            for row in table.rows:
                row_text = [cell.text.strip() for cell in row.cells if cell.text.strip()]
                if row_text:
                    texts.append(" | ".join(row_text))
        content = "\n".join(texts).strip()
        if content:
            return content
    except Exception:
        pass
    try:
        text = textract.process(path)
        return text.decode("utf-8", errors="ignore")
    except Exception:
        return ""
    
def extract_text_from_txt(path):
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

def extract_text_from_doc_or_rtf(path):
    try:
        text = textract.process(path)
        return text.decode("utf-8", errors="ignore")
    except Exception:
        return ""

def extract_text(file_path):
    ext = file_path.lower()
    if ext.endswith(".pdf"):
        return extract_text_from_pdf(file_path)
    elif ext.endswith(".docx"):
        return extract_text_from_docx(file_path)
    elif ext.endswith(".txt"):
        return extract_text_from_txt(file_path)
    elif ext.endswith(".doc") or ext.endswith(".rtf"):
        return extract_text_from_doc_or_rtf(file_path)
    else:
        return ""

# ---------- –ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–∏ ----------
def load_job_requirements(path):
    if not path or not os.path.exists(path):
        return []

    text = extract_text(path)
    lines = text.splitlines()

    requirements = []
    capture = False
    for line in lines:
        if "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è" in line:
            capture = True
            continue
        if capture:
            if not line.strip():
                break
            requirements.append(line.strip())
    return [r for r in requirements if r]

JOB_REQUIREMENTS = load_job_requirements(JOB_DESCRIPTION_PATH)

# ---------- –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏ ----------
PROMPT_INSTRUCTION = """
–¢—ã HR-—ç–∫—Å–ø–µ—Ä—Ç. –¢–µ–±–µ –¥–∞—ë—Ç—Å—è —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∏ —Å–ø–∏—Å–æ–∫ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏–∑ –≤–∞–∫–∞–Ω—Å–∏–∏.
–ù—É–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∫–∞–∂–¥–æ–º—É —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é –ø–æ —à–∫–∞–ª–µ –æ—Ç 1 –¥–æ 10 –∏ –¥–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π.
–í –∫–æ–Ω—Ü–µ –≤—ã–¥–∞–π –æ–±—â–∏–π –≤—ã–≤–æ–¥ –æ –∫–∞–Ω–¥–∏–¥–∞—Ç–µ –∏ –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É (—Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º, –æ–∫—Ä—É–≥–ª–∏ –¥–æ –¥–µ—Å—è—Ç—ã—Ö).

–û—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:

{
  "–û—Ü–µ–Ω–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π": {
    "–¢–µ–∫—Å—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è": {"–±–∞–ª–ª": 1-10, "–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π": "—Ç–µ–∫—Å—Ç"},
    "–¢–µ–∫—Å—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è": {"–±–∞–ª–ª": 1-10, "–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π": "—Ç–µ–∫—Å—Ç"}
  },
  "–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞": 1-10,
  "–û–±—â–∏–π –≤—ã–≤–æ–¥": "—Ç–µ–∫—Å—Ç–æ–≤–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ"
}

‚ö†Ô∏è –í–∞–∂–Ω–æ: –Ω–∏–∫–∞–∫–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤–Ω–µ JSON. –ö–ª—é—á–∞–º–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∏–º–µ–Ω–Ω–æ —Ç–µ–∫—Å—Ç—ã —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π.
"""

# ---------- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ----------
def evaluate_resume(file_path):
    if not file_path:
        return "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ–∑—é–º–µ."

    resume_text = extract_text(file_path)
    if not resume_text.strip():
        return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞."

    prompt = f"""
{PROMPT_INSTRUCTION}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏:
{json.dumps(JOB_REQUIREMENTS, ensure_ascii=False, indent=2)}

–†–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:
{resume_text}
"""

    payload = {"model": MODEL_NAME, "messages": [{"role": "user", "content": prompt}]}

    try:
        url = f"{API_BASE_URL}/chat/completions"
        resp = requests.post(url, json=payload)
        resp.raise_for_status()
        data = resp.json()
        raw_answer = data["choices"][0]["message"]["content"]

        clean_answer = strip_thinking_tags(raw_answer)

        try:
            parsed = json.loads(clean_answer)

            result = "## üìä –û—Ü–µ–Ω–∫–∞ –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º\n\n"
            scores = []
            if "–û—Ü–µ–Ω–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π" in parsed:
                for req, val in parsed["–û—Ü–µ–Ω–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π"].items():
                    score = val.get("–±–∞–ª–ª", "?")
                    comment = val.get("–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π", "")
                    result += f"**{req}:** {score} / 10 ‚Äî {comment}\n\n"
                    try:
                        scores.append(int(score))
                    except Exception:
                        pass

            # —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞
            if scores:
                avg_score = round(sum(scores) / len(scores), 1)
                result += f"## üìà –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {avg_score} / 10\n\n"

            if "–û–±—â–∏–π –≤—ã–≤–æ–¥" in parsed:
                result += f"## üìù –û–±—â–∏–π –≤—ã–≤–æ–¥\n{parsed['–û–±—â–∏–π –≤—ã–≤–æ–¥']}"

            return result.strip()
        except Exception:
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON. –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:\n\n{clean_answer}"

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ API: {e}"

# ---------- –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio ----------
with gr.Blocks() as demo:
    gr.Markdown("## üßë‚Äçüíº –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—é–º–µ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –≤–∞–∫–∞–Ω—Å–∏–∏")

    with gr.Row():
        with gr.Column():
            file_input = gr.File(
                label="–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞",
                type="filepath",
                file_types=[".pdf", ".doc", ".docx", ".txt", ".rtf"]
            )
            submit_btn = gr.Button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å")

        with gr.Column():
            output_box = gr.Markdown(label="–†–µ–∑—É–ª—å—Ç–∞—Ç")

    submit_btn.click(evaluate_resume, inputs=file_input, outputs=output_box)

if __name__ == "__main__":
    demo.launch()
