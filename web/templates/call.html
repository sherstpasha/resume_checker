{% extends "base.html" %}
{% block content %}
<h2>–°–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ —Å –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–º #{{ cid }}</h2>

<div class="call-wrap">
  <div class="call-stage">
    <div id="botIndicator" class="bot-indicator listening" title="–°–æ—Å—Ç–æ—è–Ω–∏–µ –±–æ—Ç–∞"></div>
    <video id="remoteVideo" autoplay playsinline class="call-remote"></video>
    <video id="localVideo" autoplay playsinline muted class="call-local"></video>
    <img id="personPlaceholder" src="/static/image_person.png" alt="you" class="call-local placeholder" style="display:none;">
  </div>
  <div class="call-controls">
    <button class="btn" id="joinBtn">–ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è</button>
    <button class="btn" id="leaveBtn" disabled>–û—Ç–∫–ª—é—á–∏—Ç—å—Å—è</button>
    <button class="btn" id="micToggle" disabled>–ú–∏–∫—Ä–æ—Ñ–æ–Ω: –≤—ã–∫–ª</button>
    <button class="btn" id="videoToggle" disabled>–ö–∞–º–µ—Ä–∞: –≤—ã–∫–ª</button>
    <span id="status" class="muted"></span>
  </div>
  <div id="chat" class="card" style="max-width:960px;">
    <div id="log" style="min-height:120px; max-height:220px; overflow:auto;"></div>
  </div>
  <div class="muted">–î–∞–π—Ç–µ –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∏ (–ø–æ –∂–µ–ª–∞–Ω–∏—é) –∫ –∫–∞–º–µ—Ä–µ.</div>
  <input type="hidden" id="cid" value="{{ cid }}">
</div>
<script>
  let pc;
  let ws;
  let sttWS;
  let recog;
  let isSpeaking = false;      // half-duplex: true while bot TTS plays
  let userMicEnabled = true;   // user preference for mic
  let readyToListen = false;   // start listening only after first bot reply
  let firstAssistTimer = null; // fallback timer
  let sttHypo = ""; // current interim hypothesis
  let sttInterval = null; // periodic sender // fallback timer if no greeting arrives
  let joining = false;
  const status = (t) => document.getElementById('status').textContent = t;
  const botEl = () => document.getElementById('botIndicator');
  function setBotState(state) {
    const el = botEl();
    if (!el) return;
    el.classList.remove('speaking','thinking','listening');
    if (state === 'speaking') el.classList.add('speaking');
    else if (state === 'thinking') el.classList.add('thinking');
    else el.classList.add('listening');
  }
  function speakText(text, onStart, onEnd) {
    try { window.speechSynthesis.cancel(); } catch(_) {}
    const u = new SpeechSynthesisUtterance(text);
    u.lang = 'ru-RU'; u.rate = 1.0; u.pitch = 1.0; u.volume = 1.0;
    u.onstart = () => { try { onStart && onStart(); } catch(_) {} };
    u.onend = () => { try { onEnd && onEnd(); } catch(_) {} };
    try { window.speechSynthesis.speak(u); } catch(_) {}
  }

  async function join() {
    try {
      if (pc || joining) { return; }
      joining = true;
      document.getElementById('joinBtn').disabled = true;
      const cid = document.getElementById('cid').value;
      // websocket for transcripts/replies (ensure single instance)
      if (ws) { try { ws.close(); } catch(_) {} ws = null; }
      ws = new WebSocket((location.protocol === 'https:' ? 'wss://' : 'ws://') + location.host + `/call/ws/${cid}`);
      ws.onmessage = (ev) => {
        try {
          const msg = JSON.parse(ev.data);
          const log = document.getElementById('log');
          const div = document.createElement('div');
          div.className = msg.role === 'assistant' ? 'msg assistant' : (msg.role === 'user' ? 'msg user' : 'msg sys');
          div.textContent = (msg.role === 'assistant' ? 'ü§ñ ' : (msg.role === 'user' ? 'üë§ ' : '‚ÑπÔ∏è ')) + (msg.text || '');
          log.appendChild(div);
          log.scrollTop = log.scrollHeight;
          // Play server-side TTS audio if provided
          if (msg.role === 'assistant' && msg.audio_url) {
            try {
              const aSender = pc && pc.getSenders().find(s=>s.track && s.track.kind==='audio');
              isSpeaking = true; setBotState('speaking');
              if (aSender && aSender.track) aSender.track.enabled = false;
              try { if (recog) recog.stop(); } catch(_) {}
              const au = new Audio(msg.audio_url);
              au.onended = () => {
                isSpeaking = false; setBotState('listening');
                if (aSender && aSender.track) aSender.track.enabled = !!userMicEnabled;
                try { if (recog && userMicEnabled) recog.start(); } catch(_) {}
              };
              au.play().catch(()=>{});
            } catch(_) {}
          }
        } catch {}
      };
      // Intercept state/assistant messages to enforce half‚Äëduplex
      const _origOnMsg = ws.onmessage;
      ws.onmessage = (ev) => {
        try {
          const msg = JSON.parse(ev.data);
          // Client-side TTS: speak assistant reply and enforce half‚Äëduplex
          if (false && msg && msg.role === 'assistant' && msg.text) {
            try {
              const aSender = pc && pc.getSenders().find(s=>s.track && s.track.kind==='audio');
              setBotState('thinking'); // yellow pause until TTS actually starts
              isSpeaking = true;
              // stop recognition during speaking
              try { if (recog) recog.stop(); } catch(_) {}
              // mute upstream mic to avoid feedback
              if (aSender && aSender.track) aSender.track.enabled = false;
              speakText(msg.text,
                () => { setBotState('speaking'); },
                () => {
                  isSpeaking = false;
                  readyToListen = true; // now allow recognition flow
                  setBotState('listening');
                  if (firstAssistTimer) { try { clearTimeout(firstAssistTimer); } catch(_) {} firstAssistTimer = null; }
                  // restore mic to user's preference
                  if (aSender && aSender.track) aSender.track.enabled = !!userMicEnabled;
                  try { if (recog && userMicEnabled) recog.start(); } catch(_) {}
                }
              );
            } catch(_) {}
          }
                    if (msg && msg.role === 'state' && msg.state) {
            const st = String(msg.state).toLowerCase();
            setBotState(st);
            const aSender = pc && pc.getSenders().find(s=>s.track && s.track.kind==='audio');
            if (st === 'speaking' || st === 'thinking') {
              isSpeaking = true;
              try { if (recog) recog.stop(); } catch(_) {}
              if (aSender && aSender.track) aSender.track.enabled = false;
            } else {
              isSpeaking = false;
              readyToListen = true;
              if (aSender && aSender.track) aSender.track.enabled = !!userMicEnabled;
              try { if (recog && userMicEnabled) recog.start(); } catch(_) {}
            }
            return;
          }        } catch {}
        if (typeof _origOnMsg === 'function') { _origOnMsg(ev); }
      };
      pc = new RTCPeerConnection({ iceServers: [{ urls: 'stun:stun.l.google.com:19302' }] });

      // remote
      const remoteVideo = document.getElementById('remoteVideo');
      pc.ontrack = (ev) => {
        if (ev.streams && ev.streams[0]) remoteVideo.srcObject = ev.streams[0];
        else remoteVideo.srcObject = new MediaStream([ev.track]);
      };

      // local
      // Before media: prepare speak-first mode (yellow pause)
      isSpeaking = true; // block listening until first assistant response is spoken
      setBotState('thinking');

      // Try to get user media; if camera is unavailable, fall back to audio only
      let stream;
      try {
        // Prefer raw mic without browser DSP for better ASR
        stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: false,
            noiseSuppression: false,
            autoGainControl: false,
            channelCount: 1,
            sampleRate: 48000
          },
          video: true
        });
      } catch(e) {
        stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: false,
            noiseSuppression: false,
            autoGainControl: false,
            channelCount: 1,
            sampleRate: 48000
          },
          video: false
        });
      }
      // ensure mic is muted while waiting for first assistant speech
      try { const at0 = stream.getAudioTracks()[0]; if (at0) at0.enabled = false; } catch(_) {}
      // fallback: if greeting doesn't arrive, start listening after 5s
      try {
        if (firstAssistTimer) { clearTimeout(firstAssistTimer); firstAssistTimer = null; }
        firstAssistTimer = setTimeout(() => {
          try {
            if (isSpeaking) {
              isSpeaking = false;
              readyToListen = true;
              setBotState('listening');
              try { const a0 = stream.getAudioTracks()[0]; if (a0) a0.enabled = !!userMicEnabled; } catch(_) {}
              try { if (recog && userMicEnabled) recog.start(); } catch(_) {}
            }
          } catch(_) {}
        }, 5000);
      } catch(_) {}
      const localVideo = document.getElementById('localVideo');
      const placeholder = document.getElementById('personPlaceholder');
      // ensure camera starts disabled if present
      try { const vt0 = stream.getVideoTracks()[0]; if (vt0) vt0.enabled = false; } catch(_) {}
      localVideo.srcObject = stream;
      const _vt = stream.getVideoTracks()[0] || null;
      if (_vt && _vt.enabled) { localVideo.style.display = ''; placeholder.style.display = 'none'; }
      else { localVideo.style.display = 'none'; placeholder.style.display = ''; }
      stream.getTracks().forEach(t => pc.addTrack(t, stream));

      // mic toggle
      const micBtn = document.getElementById('micToggle');
      const audioTrack = stream.getAudioTracks()[0] || null;
      micBtn.disabled = !audioTrack;
      function updateMic() {
        if (!audioTrack) return;
        micBtn.textContent = '–ú–∏–∫—Ä–æ—Ñ–æ–Ω: ' + (audioTrack.enabled ? '–≤–∫–ª' : '–≤—ã–∫–ª');
      }
      micBtn.onclick = () => { if (audioTrack) { audioTrack.enabled = !audioTrack.enabled; updateMic(); micBtn.classList.toggle('btn-danger', !audioTrack.enabled); } };
      updateMic();
      micBtn.classList.toggle('btn-danger', !(audioTrack && audioTrack.enabled));

      // Mic-driven animation (RMS ‚Üí scale for listening state)
      try {
        const audioTracks = stream.getAudioTracks();
        if (audioTracks && audioTracks.length) {
          const ctx = new (window.AudioContext || window.webkitAudioContext)();
          const src = ctx.createMediaStreamSource(stream);
          const analyser = ctx.createAnalyser();
          analyser.fftSize = 2048;
          const data = new Float32Array(analyser.fftSize);
          src.connect(analyser);
          let rafId;
          let smooth = 1.0;
          const render = () => {
            analyser.getFloatTimeDomainData(data);
            // RMS
            let sum = 0;
            for (let i = 0; i < data.length; i++) { const v = data[i]; sum += v*v; }
            const rms = Math.sqrt(sum / data.length);
            const el = document.getElementById('botIndicator');
            if (el) {
              if (el.classList.contains('listening')) {
                // Map RMS (~0..0.3) ‚Üí target scale 1.00..1.06 (clamped)
                const amp = Math.min(0.06, rms * 0.4); // 0..~0.12 ‚Üí clamp 0.06
                const target = 1.0 + amp;
                // Smooth step (low-pass filter)
                smooth = smooth * 0.85 + target * 0.15;
                el.style.setProperty('--levelScale', smooth.toFixed(3));
              } else {
                smooth = 1.0;
                el.style.setProperty('--levelScale', '1.000');
              }
            }
            rafId = requestAnimationFrame(render);
          };
          render();
          // cleanup on leave
          document.getElementById('leaveBtn').addEventListener('click', () => {
            try { cancelAnimationFrame(rafId); ctx.close(); } catch(_) {}
          }, { once: true });
        }
      } catch(e) { /* ignore */ }

      // video toggle (show placeholder when off)
      const videoBtn = document.getElementById('videoToggle');
      const videoTrack = stream.getVideoTracks()[0] || null;
      videoBtn.disabled = !videoTrack;
      function updateVideo() {
        if (!videoTrack) return;
        videoBtn.textContent = '–ö–∞–º–µ—Ä–∞: ' + (videoTrack.enabled ? '–≤–∫–ª' : '–≤—ã–∫–ª');
        if (videoTrack.enabled) { localVideo.style.display = ''; placeholder.style.display = 'none'; }
        else { localVideo.style.display = 'none'; placeholder.style.display = ''; }
      }
      videoBtn.onclick = () => { if (videoTrack) { videoTrack.enabled = !videoTrack.enabled; updateVideo(); videoBtn.classList.toggle('btn-danger', !videoTrack.enabled); } };
      updateVideo();
      videoBtn.classList.toggle('btn-danger', !(videoTrack && videoTrack.enabled));

      // Start browser speech recognition (Chromium Web Speech API)
      try {
        if (sttWS) { try { sttWS.close(); } catch(_) {} sttWS = null; }
        sttWS = new WebSocket((location.protocol === 'https:' ? 'wss://' : 'ws://') + location.host + `/call/stt/${cid}`);
        sttWS.onopen = () => {          // periodic interim sender (every 2s)
          try { if (sttInterval) { clearInterval(sttInterval); sttInterval = null; } } catch(_) {}
          sttInterval = setInterval(() => {
            try {
              if (!sttWS || sttWS.readyState !== 1) return;
              if (!readyToListen || isSpeaking || !userMicEnabled) return;
              const text = (sttHypo || '').trim();
              if (text) {
                sttWS.send(JSON.stringify({ text, interim: true }));
              }
            } catch(_) {}
          }, 2000);
          const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
          if (!SR) { console.warn('Web Speech API not supported'); return; }
          recog = new SR();
          recog.lang = 'ru-RU';
          recog.continuous = true;
          recog.interimResults = false;
          let enabled = true;
          const micBtn = document.getElementById('micToggle');
          const updateMicLabel = () => micBtn.textContent = '–ú–∏–∫—Ä–æ—Ñ–æ–Ω: ' + (enabled ? '–í–∫–ª' : '–í—ã–∫–ª');
          micBtn.onclick = () => {
            enabled = !enabled;
            micBtn.classList.toggle('btn-danger', !enabled);
            updateMicLabel();
            try { if (enabled) recog.start(); else recog.stop(); } catch(_) {}
          };
          updateMicLabel();
          recog.onresult = (ev) => {
  for (let i = ev.resultIndex; i < ev.results.length; i++) {
    const res = ev.results[i];
    const txt = (res[0] && res[0].transcript ? res[0].transcript : '').trim();
    if (!txt) continue;
    if (res.isFinal) {
      try { sttWS.send(JSON.stringify({ text: txt })); } catch(_) {}
      sttHypo = '';
    } else {
      sttHypo = txt;
    }
  }
};
          recog.onend = () => { if (enabled) { try { if (userMicEnabled && readyToListen &&  !isSpeaking) recog.start(); } catch(_) {}  } };
          try { if (userMicEnabled && readyToListen &&  !isSpeaking) recog.start(); } catch(_) {} 

          // Half‚Äëduplex overrides: ensure bot never listens while speaking
          try {
            const micBtn2 = document.getElementById('micToggle');
            const updateMicLabel2 = () => micBtn2.textContent = '–ú–∏–∫—Ä–æ—Ñ–æ–Ω: ' + (userMicEnabled ? '–í–∫–ª' : '–í—ã–∫–ª');
            micBtn2.onclick = () => {
              userMicEnabled = !userMicEnabled;
              micBtn2.classList.toggle('btn-danger', !userMicEnabled);
              updateMicLabel2();
              const aSender = pc && pc.getSenders().find(s=>s.track && s.track.kind==='audio');
              if (aSender && aSender.track) aSender.track.enabled = !!userMicEnabled && !isSpeaking;
              try { if (recog) { if (userMicEnabled && !isSpeaking) recog.start(); else recog.stop(); } } catch(_) {}
            };
            updateMicLabel2();
            // Only forward final results when not speaking
            recog.onresult = (ev) => {
              for (let i = ev.resultIndex; i < ev.results.length; i++) {
                const res = ev.results[i];
                if (res.isFinal) {
                  const text = (res[0] && res[0].transcript ? res[0].transcript : '').trim();
                  if (text && !isSpeaking && userMicEnabled) {
                    try { sttWS.send(JSON.stringify({ text })); } catch(_) {}
                  }
                }
              }
            };
            recog.onend = () => { if (userMicEnabled && !isSpeaking) { try { if (userMicEnabled && readyToListen &&  !isSpeaking) recog.start(); } catch(_) {}  } };
          } catch(_) {}
        };
      } catch (e) { console.warn('STT init failed', e); }

      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);

      const resp = await fetch(`/webrtc/offer/${cid}`, {
        method: 'POST', headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ sdp: pc.localDescription.sdp, type: pc.localDescription.type })
      });
      const data = await resp.json();
      if (!pc.currentRemoteDescription) {
        await pc.setRemoteDescription(new RTCSessionDescription(data));
      }
      joining = false;

      document.getElementById('joinBtn').disabled = true;
      document.getElementById('leaveBtn').disabled = false;
      document.getElementById('micToggle').disabled = false;
      if (videoTrack) document.getElementById('videoToggle').disabled = false;
      status('–ü–æ–¥–∫–ª—é—á–µ–Ω–æ');
    } catch (e) {
      try { if (pc) { pc.getSenders().forEach(s => s.track && s.track.stop()); pc.close(); } } catch(_) {}
      pc = null;
      try { if (ws) { ws.close(); } } catch(_) {}
      ws = null;
      joining = false;
      document.getElementById('joinBtn').disabled = false;
      status('–û—à–∏–±–∫–∞: ' + (e && e.message ? e.message : e));
    }
  }

  function leave() {
    try {
      const cid = document.getElementById('cid').value;
      // notify server to finish interview
      fetch(`/call/finish/${cid}`, { method: 'POST' }).catch(()=>{});
      if (pc) {
        pc.getSenders().forEach(s => s.track && s.track.stop());
        pc.close();
        pc = null;
      }
      try { if (ws) { ws.close(); } } catch(_) {}
      ws = null;
      try { if (sttWS) { sttWS.close(); } } catch(_) {}
      sttWS = null;
      try { if (sttInterval) { clearInterval(sttInterval); sttInterval = null; } } catch(_) {}
      try { if (recog) { recog.stop(); } } catch(_) {}
      recog = null;
    } finally {
      document.getElementById('joinBtn').disabled = false;
      document.getElementById('leaveBtn').disabled = true;
      // disable toggles when not connected
      const micBtn = document.getElementById('micToggle');
      const videoBtn = document.getElementById('videoToggle');
      micBtn.disabled = true; micBtn.textContent = '–ú–∏–∫—Ä–æ—Ñ–æ–Ω: –≤—ã–∫–ª';
      if (videoBtn) { videoBtn.disabled = true; videoBtn.textContent = '–ö–∞–º–µ—Ä–∞: –≤—ã–∫–ª'; }
      status('–û—Ç–∫–ª—é—á–µ–Ω–æ');
    }
  }

  document.getElementById('joinBtn').onclick = join;
  document.getElementById('leaveBtn').onclick = leave;
</script>
{% endblock %}
